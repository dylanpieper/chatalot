---
title: "Compare Packages for Batching LLM Tasks"
output: rmarkdown::html_vignette
vignette: |
  %\VignetteIndexEntry{Comparing Batching Options}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

This vignette compares different approaches to batching LLM operations in R. There are three main packages that support batch processing:

-   [hellmer](https://github.com/dylanpieper/hellmer): Synchronous batch processing for any models supported by [ellmer](https://github.com/tidyverse/ellmer/)
-   [tidyllm](https://github.com/edubruell/tidyllm): Asynchronous batch processing for Anthropic, OpenAI, or Mistral models
-   [mall](https://mlverse.github.io/mall): Local batch processing with Ollama models

## Package Comparison

### hellmer

Focuses on synchronous batch processing:

```{r eval=FALSE}
library(hellmer)

chat <- chat_batch(chat_claude, system_prompt = "Reply concisely")
result <- chat$batch(list("What is 2+2?",
                          "Name one planet.",
                          "Is water wet?",
                          "What color is the sky?"))
result$progress()
result$texts()
result$chats()
```

Key features:

-   Ellmer's [tooling](https://ellmer.tidyverse.org/articles/tool-calling.html) and [structured data extraction](https://ellmer.tidyverse.org/articles/structured-data.html)
-   State persistence and recovery
-   Progress tracking
-   Configurable output verbosity
-   Automatic retry with backoff
-   Timeout handling
-   Sound notifications

### tidyllm

Focuses on asynchronous batch processing:

```{r eval=FALSE}
library(tidyllm)

glue("Write a poem about {x}", x=c("cats","dogs","hamsters")) |>
  purrr::map(llm_message) |>
  send_batch(claude()) |>
  saveRDS("claude_batch.rds")

readRDS("claude_batch.rds") |>
   check_batch(claude())
```

Key features:

-   Asynchronous processing
-   Supports Anthropic, OpenAI, or Mistral batching APIs
-   Status checking capabilities
-   Cost savings (\~50% cheaper)

### mall

Provides synchronous batching for local Ollama models:

```{r eval=FALSE}
library(mall)

result <- data |>
  llm_sentiment(text)
```

Key features:

-   Row-wise dataframe processing
-   Integrated caching
-   Pre-built NLP task prompts
-   Pipe integration

## When to Use Each Package

-   Use [hellmer](https://github.com/dylanpieper/hellmer) for faster, but more costly, synchronous batch processing with the most provider coverage and additional tools
-   Use [tidyllm](https://github.com/edubruell/tidyllm) for slower, but less costly, asynchronous batch processing for Anthropic, OpenAI, or Mistral models
-   Use [mall](https://mlverse.github.io/mall) for local Ollama models and pre-built NLP tasks

## Performance Considerations

-   **Synchronous** ([hellmer](https://github.com/dylanpieper/hellmer), [mall](https://mlverse.github.io/mall)): Better for immediate feedback and structured tasks, blocks R until completion and consumes local resources
-   **Asynchronous** ([tidyllm](https://github.com/edubruell/tidyllm)): Better for long-running jobs and cost savings, requires status checking, does not block R and outsources batching compute
