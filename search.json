[{"path":"https://dylanpieper.github.io/chatalot/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 chatalot authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://dylanpieper.github.io/chatalot/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dylan Pieper. Author, maintainer.","code":""},{"path":"https://dylanpieper.github.io/chatalot/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Pieper D (2025). chatalot: Process Lot LLM Chats. R package version 0.2.0, https://dylanpieper.github.io/chatalot/.","code":"@Manual{,   title = {chatalot: Process a Lot of LLM Chats},   author = {Dylan Pieper},   year = {2025},   note = {R package version 0.2.0},   url = {https://dylanpieper.github.io/chatalot/}, }"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"chatalot-","dir":"","previous_headings":"","what":"Process a Lot of LLM Chats","title":"Process a Lot of LLM Chats","text":"chatalot synchronously processes lot large language model chats R using ellmer. Easily setup sequential parallel processing workflows mixed content (images PDFs), tool calling, structured data extraction, save resume, sound notifications, . chatalot similar existing ellmer tools: ellmer::parallel_chat() - Synchronously processes lot chats parallel. tool simple fast limited features option save data resume interrupted. ellmer::batch_chat() - Asynchronously batch processes lot chats select providers. tool 50% cheaper wait 24 hours response.","code":""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Process a Lot of LLM Chats","text":"can install development CRAN version package :","code":"# pak::pak(\"dylanpieper/chatalot\") pak::pak(\"chatalot\")"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"setup-api-keys","dir":"","previous_headings":"","what":"Setup API Keys","title":"Process a Lot of LLM Chats","text":"API keys allow access chat models stored environmental variables. recommend usethis package setup API keys .Renviron OPENAI_API_KEY=-key:","code":"usethis::edit_r_environ(scope = c(\"user\", \"project\"))"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"basic-usage","dir":"","previous_headings":"","what":"Basic Usage","title":"Process a Lot of LLM Chats","text":"following examples, define chat object reuse:","code":"openai <- chat_openai(system_prompt = \"Reply concisely, one sentence\")"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"sequential-processing","dir":"","previous_headings":"Basic Usage","what":"Sequential Processing","title":"Process a Lot of LLM Chats","text":"Sequential processing requests one chat time. Sequential processing slow safe, can save response, one time: Access responses:","code":"library(chatalot)  chat <- chat_sequential(openai)  prompts <- c(   \"What roles do people have in a castle?\",   \"Why are castles needed?\",   \"When was the first castle built?\",   \"Where are most castles located?\" )  response <- chat$process(prompts) response$texts() #> [1] \"In a castle, people served as rulers, warriors, administrators,  #> craftsmen, and servants who managed its defense, governance, and daily upkeep.\" #>  #> [2] \"Castles have historically been built for defense and power consolidation, #> and today they serve as cultural landmarks that preserve our heritage  #> and attract tourism.\" #>  #> [3] \"There isn’t a definitive \\\"first castle,\\\" but the earliest structures #> resembling castles emerged in medieval Europe around the 9th century.\" #>  #> [4] \"Most castles are located in Europe, particularly in historically #> turbulent regions like the United Kingdom, France, and Germany.\""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"parallel-processing","dir":"","previous_headings":"Basic Usage","what":"Parallel Processing","title":"Process a Lot of LLM Chats","text":"Parallel processing requests multiple chats time across multiple R processes using future: Chats distributed across processes chunks (default: process 10 prompts time). chunk checkpoint capturing responses multiple R processes. chunk finished, responses saved disk. fastest processing, set chunk_size number prompts: using length(prompts), aware data saved disk chats processed, risking data loss additional cost.","code":"chat <- chat_future(openai) response <- chat$process(   prompts,    chunk_size = length(prompts) )"},{"path":[]},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"mixed-content","dir":"","previous_headings":"Features","what":"Mixed Content","title":"Process a Lot of LLM Chats","text":"Process prompts combine text content (e.g., images PDFs) using ellmer content functions:","code":"library(chatalot)  chat <- chat_sequential(openai)  base_prompt <- \"What do you see in the image?\" complex_prompts <- list(   c(base_prompt, content_image_url(\"https://www.r-project.org/Rlogo.png\")),   c(base_prompt, content_image_file(system.file(\"httr2.png\", package = \"ellmer\"))) )  response <- chat$process(complex_prompts)  response$texts() #> [[1]] #> [1] \"The image shows the logo for R, a programming language and software environment  #> used for statistical computing and graphics, featuring a stylized blue \\\"R\\\"  #> inside a gray oval or ring.\"  #> [[2]] #> [1] \"The image shows a logo for \\\"httr2\\\" featuring a stylized red baseball batter #> silhouette on a dark blue hexagonal background.\""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"tool-calling","dir":"","previous_headings":"Features","what":"Tool Calling","title":"Process a Lot of LLM Chats","text":"Register use tool calling let LLM use R functions:","code":"weather <- data.frame(   city = c(\"Chicago\", \"New York\", \"Lisbon\"),   raining = c(\"Heavy\", \"None\", \"Overcast\"),   temperature = c(\"Cool\", \"Hot\", \"Warm\"),   wind = c(\"Strong\", \"Weak\", \"Strong\") )  get_weather <- function(cities) weather[weather$city %in% cities, ]  chat$register_tool(tool(   get_weather,   \"Report on weather conditions.\",   cities = type_array(\"City names\", type_string()) ))  response <- chat$process(interpolate(\"Brief weather update for {{weather$city}}?\"))  response$texts() #> [1] \"Chicago is experiencing heavy rain, cool temperatures, and strong winds.\" #> [2] \"New York is experiencing hot conditions with no rain and light winds.\" #> [3] \"In Lisbon, the weather is overcast with warm temperatures and strong winds.\""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"structured-data-extraction","dir":"","previous_headings":"Features","what":"Structured Data Extraction","title":"Process a Lot of LLM Chats","text":"Extract structured data using type specifications:","code":"prompts <- c(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" )  response <- chat$process(   prompts,   type = type_object(     name = type_string(),     age = type_number()   ) )  response$texts() #>     name age #> 1   Alex  42 #> 2  Jamal  27 #> 3 Li Wei  19 #> 4 Fatima  35 #> 5 Robert  51 #> 6  Kwame  50"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"save-and-resume","dir":"","previous_headings":"Features","what":"Save and Resume","title":"Process a Lot of LLM Chats","text":"Progress tracked response$progress() saved .rds file disk, allows easily resume interrupted operations: file defined, temporary file created default.","code":"response <- chat$process(prompts, file = \"chat.rds\")"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"sound-notifications","dir":"","previous_headings":"Features","what":"Sound Notifications","title":"Process a Lot of LLM Chats","text":"Toggle sound notifications completion, interruption, error:","code":"response <- chat$process(prompts, beep = TRUE)"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"verbosity-options","dir":"","previous_headings":"Features","what":"Verbosity Options","title":"Process a Lot of LLM Chats","text":"default, chat echo set FALSE show progress bar. However, can still configure echo first setting progress FALSE:","code":"prompts <- c(   \"What is R?\",   \"Explain base R versus tidyverse\" )  response <- chat$process(   prompts,   progress = FALSE,   echo = TRUE ) #> R is a programming language and software environment used for  #> statistical computing and graphics. #>  #> Base R consists of the core functionalities built into R,  #> while tidyverse is a collection of packages that offer a more #> consistent, readable, and streamlined approach to data manipulation,  #> visualization, and analysis."},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"methods","dir":"","previous_headings":"Features","what":"Methods","title":"Process a Lot of LLM Chats","text":"progress(): Returns processing status texts(): Returns response texts format input prompts (.e., list prompts provided list, character vector prompts provided vector). type provided, list one element prompt. type consistent object, returns data frame one row prompt, one column property. chats(): Returns list chat objects","code":""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"extras","dir":"","previous_headings":"","what":"Extras","title":"Process a Lot of LLM Chats","text":"Batch Compare Similarity LLM Responses R (Blog Post)","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_future.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts in parallel — chat_future","title":"Process a lot of prompts in parallel — chat_future","text":"Processes lot chat prompts using parallel workers. Splits prompts chunks processing maintaining state. sequential processing, use chat_sequential().","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_future.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts in parallel — chat_future","text":"","code":"chat_future(chat_model = NULL, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_future.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts in parallel — chat_future","text":"chat_model ellmer chat model object function (e.g., chat_openai()) ... Additional arguments passed underlying chat model (e.g., system_prompt)","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_future.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts in parallel — chat_future","text":"process object (S7 class) containing: prompts: Original input prompts responses: Raw response data completed prompts completed: Number successfully processed prompts file: Path batch state saved type: Type specification used structured data texts: Function extract text responses structured data chats: Function extract chat objects progress: Function get processing status process: Function process lot prompts","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_future.html","id":"process-method","dir":"Reference","previous_headings":"","what":"Process Method","title":"Process a lot of prompts in parallel — chat_future","text":"function provides access process() method parallel processing prompts. See ?process.future_chat full details method parameters.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_future.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process a lot of prompts in parallel — chat_future","text":"","code":"if (FALSE) { # interactive() && ellmer::has_credentials(\"openai\") # Create chat processor chat <- chat_future(chat_openai(system_prompt = \"Reply concisely\"))  # Process prompts response <- chat$process(   list(     \"What is R?\",     \"Explain base R versus tidyverse\",     \"Explain vectors, lists, and data frames\"   ) )  # Return responses response$texts()  # Return chat objects response$chats()  # Check progress if interrupted response$progress() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts in sequence — chat_sequential","title":"Process a lot of prompts in sequence — chat_sequential","text":"Processes lot chat prompts one time sequential order. Maintains state runs can resume interrupted processing. parallel processing, use chat_future().","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts in sequence — chat_sequential","text":"","code":"chat_sequential(chat_model = NULL, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts in sequence — chat_sequential","text":"chat_model ellmer chat model object function (e.g., chat_openai()) ... Additional arguments passed underlying chat model (e.g., system_prompt)","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_sequential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts in sequence — chat_sequential","text":"process object (S7 class) containing prompts: Original input prompts responses: Raw response data completed prompts completed: Number successfully processed prompts file: Path batch state saved type: Type specification used structured data texts: Function extract text responses structured data chats: Function extract chat objects progress: Function get processing status process: Function process lot prompts","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_sequential.html","id":"process-method","dir":"Reference","previous_headings":"","what":"Process Method","title":"Process a lot of prompts in sequence — chat_sequential","text":"function provides access process() method sequential processing prompts. See ?process.sequential_chat full details method parameters.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chat_sequential.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process a lot of prompts in sequence — chat_sequential","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create chat processor chat <- chat_sequential(chat_openai(system_prompt = \"Reply concisely\"))  # Process prompts response <- chat$process(   list(     \"What is R?\",     \"Explain base R versus tidyverse\",     \"Explain vectors, lists, and data frames\"   ) )   # Return responses response$texts()  # Return chat objects response$chats()  # Check progress if interrupted response$progress() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract chat objects from a process response — chats","title":"Extract chat objects from a process response — chats","text":"Extract chat objects process response","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract chat objects from a process response — chats","text":"","code":"chats(x, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract chat objects from a process response — chats","text":"x process object ... Additional arguments","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract chat objects from a process response — chats","text":"list chat objects","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract chat objects from a process response — chats","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- chat_sequential(chat_openai())  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Return the chat objects response$chats() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts with a parallel chat — process.future_chat","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"Process lot prompts parallel chat","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"","code":"process.future_chat(   chat_env,   prompts,   type = NULL,   file = tempfile(\"chat_\", fileext = \".rds\"),   workers = NULL,   chunk_size = 10,   max_chunk_attempts = 3L,   beep = TRUE,   progress = TRUE,   echo = FALSE,   ... )"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"chat_env chat environment chat_future prompts List prompts process type Type specification structured data extraction file Path save state file workers Number parallel workers (default upper limit parallel::detectCores()) chunk_size Number prompts worker processes time (default: 10) max_chunk_attempts Maximum retries per failed chunk beep Whether play sound completion progress Whether show progress bars echo Whether display chat outputs (progress FALSE) ... Additional arguments passed chat method","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"process object processed results","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process response class for managing chat processing — process","title":"Process response class for managing chat processing — process","text":"Process response class managing chat processing","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process response class for managing chat processing — process","text":"","code":"process(   prompts = list(),   responses = list(),   completed = integer(0),   file = character(0),   type = NULL,   progress = logical(0),   input_type = character(0),   chunk_size = integer(0),   workers = integer(0),   beep = logical(0),   echo = logical(0),   state = list() )"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process response class for managing chat processing — process","text":"prompts List prompts process responses List store responses completed Integer indicating number completed prompts file Path save state file (.rds) type Type specification structured data extraction progress Whether show progress bars (default: TRUE) input_type Type input (\"vector\" \"list\") chunk_size Size chunks parallel processing workers Number parallel workers beep Play sound completion (default: TRUE) echo Whether echo messages processing (default: FALSE) state Internal state tracking","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process response class for managing chat processing — process","text":"Returns S7 class object class \"process\" represents collection prompts responses chat models. object contains input parameters properties provides methods : Extracting text responses via texts() (includes structured data type specification provided) Accessing full chat objects via chats() Tracking processing progress via progress() process object manages prompt processing tracks completion status.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process response class for managing chat processing — process","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- chat_sequential(chat_openai())  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Check the progress if interrupted response$progress()  # Return the responses as a vector or list response$texts()  # Return the chat objects response$chats() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts with a sequential chat — process.sequential_chat","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"Process lot prompts sequential chat","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"","code":"process.sequential_chat(   chat_env,   prompts,   type = NULL,   file = tempfile(\"chat_\", fileext = \".rds\"),   progress = TRUE,   beep = TRUE,   echo = FALSE,   ... )"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"chat_env chat environment chat_sequential prompts List prompts process type Type specification structured data extraction file Path save state file (.rds) progress Whether show progress bars beep Whether play sound completion echo Whether display chat outputs (progress FALSE) ... Additional arguments passed chat method","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"process object processed results","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":null,"dir":"Reference","previous_headings":"","what":"Get progress information from a process response — progress","title":"Get progress information from a process response — progress","text":"Get progress information process response","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get progress information from a process response — progress","text":"","code":"progress(x, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get progress information from a process response — progress","text":"x process object ... Additional arguments passed methods","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get progress information from a process response — progress","text":"list containing progress details","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get progress information from a process response — progress","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- chat_sequential(chat_openai())  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Check the progress response$progress() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract texts or structured data from a process response — texts","title":"Extract texts or structured data from a process response — texts","text":"Extract texts structured data process response","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract texts or structured data from a process response — texts","text":"","code":"texts(x, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract texts or structured data from a process response — texts","text":"x process object ... Additional arguments passed methods","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract texts or structured data from a process response — texts","text":"character vector list text responses. type specification provided batch, return structured data.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract texts or structured data from a process response — texts","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- chat_sequential(chat_openai())  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Extract text responses response$texts() }"},{"path":[]},{"path":"https://dylanpieper.github.io/chatalot/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"chatalot 0.2.0","text":"Added support prompts mixed content (text, images, files) using ellmer content functions","code":""},{"path":"https://dylanpieper.github.io/chatalot/news/index.html","id":"internal-improvements-0-2-0","dir":"Changelog","previous_headings":"","what":"Internal Improvements","title":"chatalot 0.2.0","text":"$texts() returns data frame structured data possible Added single retry failed structured data extractions Replaced progress bar percentage indicator (cli::pb_percent) estimated time completion (cli::pb_eta) chat_future(), updated default chunk_size fixed value 10 prompts chat_future(), detect ellmer > 0.2.0 abort due API key redaction (temporary callback added)","code":""},{"path":"https://dylanpieper.github.io/chatalot/news/index.html","id":"lifecycle-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Lifecycle changes","title":"chatalot 0.2.0","text":"Renamed package hellmer chatalot lot renamed process consistent verb use batch renamed lot match new package name state_path renamed file consistency/simplicity type_spec renamed type following latest update ellmer (0.1.1) Removed evaluation functionality poor performance Removed retry functionality anticipation robust changes ellmer (development)","code":""}]
