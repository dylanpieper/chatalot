[{"path":"https://dylanpieper.github.io/chatalot/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 chatalot authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://dylanpieper.github.io/chatalot/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Dylan Pieper. Author, maintainer.","code":""},{"path":"https://dylanpieper.github.io/chatalot/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Pieper D (2025). chatalot: Process Lot LLM Chats. R package version 0.2.0, https://dylanpieper.github.io/chatalot/.","code":"@Manual{,   title = {chatalot: Process a Lot of LLM Chats},   author = {Dylan Pieper},   year = {2025},   note = {R package version 0.2.0},   url = {https://dylanpieper.github.io/chatalot/}, }"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"chatalot-","dir":"","previous_headings":"","what":"Process a Lot of LLM Chats","title":"Process a Lot of LLM Chats","text":"chatalot processes lot large language model chats R extension ellmer. Easily setup sequential parallel chat processors support tool calling, structured data extraction, uploaded content, save resume, sound notifications, .","code":""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"chatalot-or-ellmer","dir":"","previous_headings":"","what":"chatalot or ellmer?","title":"Process a Lot of LLM Chats","text":"chatalot prioritizes safety recovery, ellmer prioritizes speed cost savings.","code":""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Process a Lot of LLM Chats","text":"can install development CRAN version package :","code":"# pak::pak(\"dylanpieper/chatalot\") pak::pak(\"chatalot\")"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"setup-api-keys","dir":"","previous_headings":"","what":"Setup API Keys","title":"Process a Lot of LLM Chats","text":"API keys allow access chat models stored environmental variables. recommend usethis setup API keys .Renviron OPENAI_API_KEY=-key:","code":"usethis::edit_r_environ(scope = c(\"user\", \"project\"))"},{"path":[]},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"sequential-processing","dir":"","previous_headings":"Basic Usage","what":"Sequential Processing","title":"Process a Lot of LLM Chats","text":"Process chats sequence, one time. Save responses disk chat resume processing last saved chat. Access responses:","code":"library(chatalot)  chat <- seq_chat(\"openai/gpt-4.1\", system_prompt = \"Reply concisely, one sentence\")  prompts <- c(   \"What roles do people have in a castle?\",   \"Why are castles needed?\",   \"When was the first castle built?\",   \"Where are most castles located?\" )  response <- chat$process(prompts) response$texts() #> [1] \"In a castle, people served as rulers, warriors, administrators,  #> craftsmen, and servants who managed its defense, governance, and daily upkeep.\" #>  #> [2] \"Castles have historically been built for defense and power consolidation, #> and today they serve as cultural landmarks that preserve our heritage  #> and attract tourism.\" #>  #> [3] \"There isn’t a definitive \\\"first castle,\\\" but the earliest structures #> resembling castles emerged in medieval Europe around the 9th century.\" #>  #> [4] \"Most castles are located in Europe, particularly in historically #> turbulent regions like the United Kingdom, France, and Germany.\""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"parallel-processing","dir":"","previous_headings":"Basic Usage","what":"Parallel Processing","title":"Process a Lot of LLM Chats","text":"Parallel processing requests multiple chats time across multiple R processes using future: Splits prompts chunks distribute across workers process chats (default: process 10 prompts time). Saves chat data disk chunks can resume processing last saved chunk. fastest processing, set chunk_size number prompts: setting chunk_size length(prompts), aware data saved disk chats processed, risking data loss additional cost. may want limit number simultaneous requests meet provider’s rate limits using workers argument (default parallel::detectCores()):","code":"chat <- future_chat(\"openai/gpt-4.1\", system_prompt = \"Reply concisely, one sentence\") response <- chat$process(prompts, chunk_size = length(prompts)) response <- chat$process(prompts, workers = 4)"},{"path":[]},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"tool-calling","dir":"","previous_headings":"Features","what":"Tool Calling","title":"Process a Lot of LLM Chats","text":"Register use tool calling let LLM use R functions:","code":"weather <- data.frame(   city = c(\"Chicago\", \"New York\", \"Lisbon\"),   raining = c(\"Heavy\", \"None\", \"Overcast\"),   temperature = c(\"Cool\", \"Hot\", \"Warm\"),   wind = c(\"Strong\", \"Weak\", \"Strong\") )  get_weather <- tool(   function(cities) weather[weather$city %in% cities, ],   description = \"Report on weather conditions.\",   arguments = list(     cities = type_array(type_string(), \"City names\")   ) )  chat$register_tool(get_weather)  response <- chat$process(interpolate(\"Brief weather update for {{weather$city}}?\"))  response$texts() #> [1] \"Chicago is experiencing heavy rain, cool temperatures, and strong winds.\" #> [2] \"New York is experiencing hot conditions with no rain and light winds.\" #> [3] \"In Lisbon, the weather is overcast with warm temperatures and strong winds.\""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"structured-data-extraction","dir":"","previous_headings":"Features","what":"Structured Data Extraction","title":"Process a Lot of LLM Chats","text":"Extract structured data using type specifications:","code":"prompts <- c(   \"I go by Alex. 42 years on this planet and counting.\",   \"Pleased to meet you! I'm Jamal, age 27.\",   \"They call me Li Wei. Nineteen years young.\",   \"Fatima here. Just celebrated my 35th birthday last week.\",   \"The name's Robert - 51 years old and proud of it.\",   \"Kwame here - just hit the big 5-0 this year.\" )  response <- chat$process(   prompts,   type = type_object(     name = type_string(),     age = type_number()   ) )  response$texts() #>     name age #> 1   Alex  42 #> 2  Jamal  27 #> 3 Li Wei  19 #> 4 Fatima  35 #> 5 Robert  51 #> 6  Kwame  50"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"uploaded-content","dir":"","previous_headings":"Features","what":"Uploaded Content","title":"Process a Lot of LLM Chats","text":"Process prompts text uploaded content (e.g., images PDFs):","code":"base_prompt <- \"What do you see in the image?\" img_prompts <- list(   c(base_prompt, content_image_url(\"https://www.r-project.org/Rlogo.png\")),   c(base_prompt, content_image_file(system.file(\"httr2.png\", package = \"ellmer\"))) )  response <- chat$process(img_prompts)  response$texts() #> [[1]] #> [1] \"The image shows the logo for R, a programming language and software environment  #> used for statistical computing and graphics, featuring a stylized blue \\\"R\\\"  #> inside a gray oval or ring.\"  #> [[2]] #> [1] \"The image shows a logo for \\\"httr2\\\" featuring a stylized red baseball batter #> silhouette on a dark blue hexagonal background.\""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"save-and-resume","dir":"","previous_headings":"Features","what":"Save and Resume","title":"Process a Lot of LLM Chats","text":"interrupt chat processing (e.g., check responses) experience error, can call process() resume last saved chat chunk: file defined, temporary .rds file created default.","code":"response <- chat$process(prompts, file = \"chat.rds\")"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"sound-notifications","dir":"","previous_headings":"Features","what":"Sound Notifications","title":"Process a Lot of LLM Chats","text":"Toggle sound notifications completion, interruption, error:","code":"response <- chat$process(prompts, beep = TRUE)"},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"verbosity-options","dir":"","previous_headings":"Features","what":"Verbosity Options","title":"Process a Lot of LLM Chats","text":"default, chat echo set FALSE show progress bar. However, can still configure echo first setting progress FALSE:","code":"prompts <- c(   \"What is R?\",   \"Explain base R versus tidyverse\" )  response <- chat$process(prompts, progress = FALSE, echo = TRUE) #> R is a programming language and software environment used for  #> statistical computing and graphics. #>  #> Base R consists of the core functionalities built into R,  #> while tidyverse is a collection of packages that offer a more #> consistent, readable, and streamlined approach to data manipulation,  #> visualization, and analysis."},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"methods","dir":"","previous_headings":"Features","what":"Methods","title":"Process a Lot of LLM Chats","text":"texts(): Returns response texts format input prompts (.e., list prompts provided list, character vector prompts provided vector). type provided, returns list one element prompt. type consistent, returns data frame one row prompt, one column property. chats(): Returns list chat objects progress(): Returns processing status","code":""},{"path":"https://dylanpieper.github.io/chatalot/index.html","id":"tidbits","dir":"","previous_headings":"","what":"Tidbits","title":"Process a Lot of LLM Chats","text":"chatalot::seq_chat() chatalot::future_chat(): Allow rate limits exceeded fallback ellmer’s retry mechanism (reactive) ellmer::parallel_chat(): Throttles requests prevent rate limits (proactive) ellmer::batch_chat(): Managed API provider Batch Compare Similarity LLM Responses R (Blog Post)","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract chat objects from a process response — chats","title":"Extract chat objects from a process response — chats","text":"Extract chat objects process response","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract chat objects from a process response — chats","text":"","code":"chats(x, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract chat objects from a process response — chats","text":"x process object ... Additional arguments","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract chat objects from a process response — chats","text":"list chat objects","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/chats.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract chat objects from a process response — chats","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- seq_chat(\"openai/gpt-4.1\")  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Return the chat objects response$chats() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/future_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts in parallel — future_chat","title":"Process a lot of prompts in parallel — future_chat","text":"Process lot chat prompts parallel using future workers (multisession). Split prompts chunks distribute across workers process chats. Save chat data disk chunks resume processing last saved chunk. sequential processing, use seq_chat().","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/future_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts in parallel — future_chat","text":"","code":"future_chat(chat_model = NULL, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/future_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts in parallel — future_chat","text":"chat_model Character string specifying chat model use (e.g., \"openai/gpt-4.1\" \"anthropic/claude-3-5-sonnet-latest\"). creates ellmer chat object using ellmer::chat(). ... Additional arguments passed underlying chat model (e.g., system_prompt)","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/future_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts in parallel — future_chat","text":"process object (S7 class) containing: prompts: Original input prompts responses: Raw response data completed prompts completed: Number successfully processed prompts file: Path batch state saved type: Type specification used structured data texts: Function extract text responses structured data chats: Function extract chat objects progress: Function get processing status process: Function process lot prompts","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/future_chat.html","id":"process-method","dir":"Reference","previous_headings":"","what":"Process Method","title":"Process a lot of prompts in parallel — future_chat","text":"function provides access process() method parallel processing prompts. See ?process.future_chat full details method parameters.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/future_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process a lot of prompts in parallel — future_chat","text":"","code":"if (FALSE) { # interactive() && ellmer::has_credentials(\"openai\") # Create chat processor chat <- future_chat(\"openai/gpt-4.1\")  # Process prompts response <- chat$process(   list(     \"What is R?\",     \"Explain base R versus tidyverse\",     \"Explain vectors, lists, and data frames\"   ) )  # Return responses response$texts()  # Return chat objects response$chats()  # Check progress if interrupted response$progress() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts with a parallel chat — process.future_chat","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"Process lot prompts parallel chat","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"","code":"process.future_chat(   chat_env,   prompts,   type = NULL,   file = tempfile(\"chat_\", fileext = \".rds\"),   workers = NULL,   chunk_size = 10,   max_chunk_tries = 2L,   beep = TRUE,   progress = TRUE,   echo = FALSE,   ... )"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"chat_env chat environment future_chat prompts List prompts process type Type specification structured data extraction file Path save state file workers Number parallel workers (default upper limit parallel::detectCores()) chunk_size Number prompts worker processes time (default: 10) max_chunk_tries Maximum tries per failed chunk beep Whether play sound completion progress Whether show progress bars echo Whether display chat outputs (progress FALSE) ... Additional arguments passed chat method","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.future_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts with a parallel chat — process.future_chat","text":"process object processed results","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process response class for managing chat processing — process","title":"Process response class for managing chat processing — process","text":"Process response class managing chat processing","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process response class for managing chat processing — process","text":"","code":"process(   prompts = list(),   responses = list(),   completed = integer(0),   file = character(0),   type = NULL,   progress = logical(0),   input_type = character(0),   chunk_size = integer(0),   workers = integer(0),   beep = logical(0),   echo = logical(0),   state = list() )"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process response class for managing chat processing — process","text":"prompts List prompts process responses List store responses completed Integer indicating number completed prompts file Path save state file (.rds) type Type specification structured data extraction progress Whether show progress bars (default: TRUE) input_type Type input (\"vector\" \"list\") chunk_size Size chunks parallel processing workers Number parallel workers beep Play sound completion (default: TRUE) echo Whether echo messages processing (default: FALSE) state Internal state tracking","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process response class for managing chat processing — process","text":"Returns S7 class object class \"process\" represents collection prompts responses chat models. object contains input parameters properties provides methods : Extracting text responses via texts() (includes structured data type specification provided) Accessing full chat objects via chats() Tracking processing progress via progress() process object manages prompt processing tracks completion status.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process response class for managing chat processing — process","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- seq_chat(\"openai/gpt-4.1\")  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Check the progress if interrupted response$progress()  # Return the responses as a vector or list response$texts()  # Return the chat objects response$chats() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts with a sequential chat — process.sequential_chat","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"Process lot prompts sequential chat","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"","code":"process.sequential_chat(   chat_env,   prompts,   type = NULL,   file = tempfile(\"chat_\", fileext = \".rds\"),   progress = TRUE,   beep = TRUE,   echo = FALSE,   ... )"},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"chat_env chat environment seq_chat prompts List prompts process type Type specification structured data extraction file Path save state file (.rds) progress Whether show progress bars beep Whether play sound completion echo Whether display chat outputs (progress FALSE) ... Additional arguments passed chat method","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/process.sequential_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts with a sequential chat — process.sequential_chat","text":"process object processed results","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":null,"dir":"Reference","previous_headings":"","what":"Get progress information from a process response — progress","title":"Get progress information from a process response — progress","text":"Get progress information process response","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Get progress information from a process response — progress","text":"","code":"progress(x, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Get progress information from a process response — progress","text":"x process object ... Additional arguments passed methods","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Get progress information from a process response — progress","text":"list containing progress details","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/progress.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Get progress information from a process response — progress","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- seq_chat(\"openai/gpt-4.1\")  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Check the progress response$progress() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/seq_chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Process a lot of prompts in sequence — seq_chat","title":"Process a lot of prompts in sequence — seq_chat","text":"Process lot chat prompts sequence, one time. Save responses disk chat resume processing last saved chat. parallel processing, use future_chat().","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/seq_chat.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process a lot of prompts in sequence — seq_chat","text":"","code":"seq_chat(chat_model = NULL, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/seq_chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process a lot of prompts in sequence — seq_chat","text":"chat_model Character string specifying chat model use (e.g., \"openai/gpt-4.1\" \"anthropic/claude-3-5-sonnet-latest\"). creates ellmer chat object using ellmer::chat(). ... Additional arguments passed underlying chat model (e.g., system_prompt)","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/seq_chat.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process a lot of prompts in sequence — seq_chat","text":"process object (S7 class) containing prompts: Original input prompts responses: Raw response data completed prompts completed: Number successfully processed prompts file: Path batch state saved type: Type specification used structured data texts: Function extract text responses structured data chats: Function extract chat objects progress: Function get processing status process: Function process lot prompts","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/seq_chat.html","id":"process-method","dir":"Reference","previous_headings":"","what":"Process Method","title":"Process a lot of prompts in sequence — seq_chat","text":"function provides access process() method sequential processing prompts. See ?process.sequential_chat full details method parameters.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/seq_chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process a lot of prompts in sequence — seq_chat","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create chat processor chat <- seq_chat(\"openai/gpt-4.1\")  # Process prompts response <- chat$process(   list(     \"What is R?\",     \"Explain base R versus tidyverse\",     \"Explain vectors, lists, and data frames\"   ) )   # Return responses response$texts()  # Return chat objects response$chats()  # Check progress if interrupted response$progress() }"},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract texts or structured data from a process response — texts","title":"Extract texts or structured data from a process response — texts","text":"Extract texts structured data process response","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract texts or structured data from a process response — texts","text":"","code":"texts(x, ...)"},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract texts or structured data from a process response — texts","text":"x process object ... Additional arguments passed methods","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract texts or structured data from a process response — texts","text":"character vector list text responses. type specification provided batch, return structured data.","code":""},{"path":"https://dylanpieper.github.io/chatalot/reference/texts.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract texts or structured data from a process response — texts","text":"","code":"if (FALSE) { # ellmer::has_credentials(\"openai\") # Create a chat processor chat <- seq_chat(\"openai/gpt-4.1\")  # Process a batch of prompts response <- chat$process(list(   \"What is R?\",   \"Explain base R versus tidyverse\",   \"Explain vectors, lists, and data frames\" ))  # Extract text responses response$texts() }"},{"path":[]},{"path":"https://dylanpieper.github.io/chatalot/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"chatalot 0.2.0","text":"Depends ellmer > 0.3.0, uses new API specify particular model (e.g., chat(\"anthropic/claude-3-5-sonnet-latest\")) Added support prompts mixed content (text, images, files) using ellmer content functions","code":""},{"path":"https://dylanpieper.github.io/chatalot/news/index.html","id":"internal-improvements-0-2-0","dir":"Changelog","previous_headings":"","what":"Internal Improvements","title":"chatalot 0.2.0","text":"$texts() returns data frame structured data possible Replaced progress bar percentage indicator (cli::pb_percent) estimated time completion (cli::pb_eta)","code":""},{"path":"https://dylanpieper.github.io/chatalot/news/index.html","id":"lifecycle-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Lifecycle changes","title":"chatalot 0.2.0","text":"Renamed package hellmer chatalot lot renamed process consistent verb use batch renamed lot match new package name state_path renamed file simplicity max_chunk_retries renamed max_chunk_tries simplicity (default: 2) type_spec renamed type following latest update ellmer (0.1.1) chat_sequential() renamed seq_chat() chat_future() renamed future_chat() Removed evaluation functionality due poor performance Removed retry functionality due new robust retries ellmer (0.3.0)","code":""}]
