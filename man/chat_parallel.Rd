% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hellmer.R
\name{chat_parallel}
\alias{chat_parallel}
\title{Process a batch of prompts in parallel}
\usage{
chat_parallel(
  chat_model = ellmer::chat_claude,
  workers = 4,
  plan = "multisession",
  beep = TRUE,
  chunk_size = 4L,
  max_chunk_attempts = 3L,
  max_retries = 3L,
  initial_delay = 20,
  max_delay = 60,
  backoff_factor = 2,
  timeout = 60,
  ...
)
}
\arguments{
\item{chat_model}{Chat model function/object (default: \code{ellmer::chat_claude})}

\item{workers}{Number of parallel workers to use (default: 4)}

\item{plan}{Processing strategy to use: "multisession" for separate R sessions
or "multicore" for forked processes (default: "multisession")}

\item{beep}{Logical to play a sound on batch completion, interruption, and error (default: TRUE)}

\item{chunk_size}{Number of prompts to process in parallel at a time (default: 4)}

\item{max_chunk_attempts}{Maximum number of retry attempts for failed chunks (default: 3)}

\item{max_retries}{Maximum number of retry attempts per prompt (default: 3)}

\item{initial_delay}{Initial delay in seconds before first retry (default: 20)}

\item{max_delay}{Maximum delay in seconds between retries (default: 60)}

\item{backoff_factor}{Factor to multiply delay by after each retry (default: 2)}

\item{timeout}{Maximum time in seconds to wait for each prompt response (default: 2)}

\item{...}{Additional arguments passed to the chat model}
}
\value{
A batch results object containing:
\itemize{
\item prompts: Original input prompts
\item responses: Raw response data for completed prompts
\item completed: Number of successfully processed prompts
\item state_path: Path where batch state is saved
\item type_spec: Type specification used for structured data
\item texts: Function to extract text responses
\item chats: Function to extract chat objects
\item progress: Function to get processing status
\item structured_data: Function to extract structured data (if \code{type_spec} was provided)
}
}
\description{
Processes a batch of chat prompts using parallel workers.
Splits prompts into chunks for processing while maintaining state.
For sequential processing, use \code{chat_batch()}.
}
