% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hellmer.R
\name{chat_parallel}
\alias{chat_parallel}
\title{Process a batch of prompts in parallel}
\usage{
chat_parallel(
  chat_model = chat_openai(),
  workers = 4,
  parallel_plan = "multisession",
  beep = TRUE,
  ...
)
}
\arguments{
\item{chat_model}{A chat model object (default: chat_openai())}

\item{workers}{Number of parallel workers to use (default: 4)}

\item{parallel_plan}{Processing strategy to use: "multisession" for separate R sessions
or "multicore" for forked processes (default: "multisession")}

\item{beep}{Logical indicating whether to play sound on completion (default: TRUE)}

\item{...}{Additional arguments passed to the chat model}
}
\value{
A batch results object containing:
\itemize{
\item prompts: Original input prompts
\item responses: Raw response data for completed prompts
\item completed: Number of successfully processed prompts
\item state_path: Path where batch state is saved
\item type_spec: Type specification used for structured data
\item texts(): Function to extract text responses
\item chats(): Function to extract chat objects
\item progress(): Function to get processing status
\item structured_data(): Function to extract structured data if type_spec was provided
}
}
\description{
Processes a batch of chat prompts using parallel workers.
Splits prompts into chunks for processing while maintaining state.
For sequential processing, use \code{chat_batch()}.
}
